{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f34f307b1cd08a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, AdamW\n",
    "import json\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ====== –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã ======\n",
    "MODEL_NAME = \"t5-base\"  # –ú–æ–∂–Ω–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å \"t5-large\"\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 5e-5\n",
    "MAX_LENGTH = 1024\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ],
   "id": "76f01a1853c80ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ====== –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞—Ç–∞—Å–µ—Ç—ã ======\n",
    "with open(\"datasets/processed_train.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open(\"datasets/processed_dev.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    val_data = json.load(f)"
   ],
   "id": "d5dc58950a7abc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ====== –û–ø—Ä–µ–¥–µ–ª—è–µ–º Dataset ======\n",
    "class SQLDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=MAX_LENGTH):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        input_text = f\"Schema: {item['schema']}\\nQuery: {item['question']}\\nSQL:\"\n",
    "        \n",
    "        input_encodings = self.tokenizer(\n",
    "            input_text, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        target_encodings = self.tokenizer(\n",
    "            item[\"sql_query\"], max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_encodings[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": input_encodings[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": target_encodings[\"input_ids\"].squeeze(),\n",
    "        }"
   ],
   "id": "38fe8a47a124b11a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ====== –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ –º–æ–¥–µ–ª—å ======\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME).to(DEVICE)"
   ],
   "id": "c7a8faf7fb0e01ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ====== –ì–æ—Ç–æ–≤–∏–º DataLoader ======\n",
    "train_dataset = SQLDataset(train_data, tokenizer)\n",
    "val_dataset = SQLDataset(val_data, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)"
   ],
   "id": "1183b4c2d69a45ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ====== –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä ======\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)"
   ],
   "id": "8fa97cd9ec5e6c27"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ====== –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ—Ç–µ—Ä—å ======\n",
    "train_losses = []\n",
    "val_losses = []"
   ],
   "id": "8d78e3cfbbc0b0eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ====== –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ ======\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            total_val_loss += outputs.loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–∏\n",
    "    model.save_pretrained(f\"t5_text2sql_epoch{epoch+1}\")\n",
    "    tokenizer.save_pretrained(f\"t5_text2sql_epoch{epoch+1}\")"
   ],
   "id": "e918a974c0dfdccc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ====== –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞ ======\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, EPOCHS+1), train_losses, label=\"Train Loss\", marker=\"o\")\n",
    "plt.plot(range(1, EPOCHS+1), val_losses, label=\"Validation Loss\", marker=\"s\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(\"training_loss_plot.png\")\n",
    "plt.show()"
   ],
   "id": "90da1cafcbdee7bb"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ====== –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª–æ–≥–æ–≤ –≤ CSV ======\n",
    "df = pd.DataFrame({\"Epoch\": list(range(1, EPOCHS+1)), \"Train Loss\": train_losses, \"Validation Loss\": val_losses})\n",
    "df.to_csv(\"training_log.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –∫–∞–∫ training_loss_plot.png üéâ\")\n"
   ],
   "id": "initial_id"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
